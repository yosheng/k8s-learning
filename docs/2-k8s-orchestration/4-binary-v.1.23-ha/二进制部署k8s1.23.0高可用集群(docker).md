### 本资源由 itjc8.com 收集整理
# 第一章：k8s介绍及集群环境部署

#### kubernetes介绍

![](二进制部署k8s1.23.0高可用集群(docker).accets/1677739343172-1682841162231.png)

kubernetes（k8s）是2015年由Google公司基于Go语言编写的一款开源的容器集群编排系统，用于自动化容器的部署、扩缩容和管理；

kubernetes（k8s）是基于Google内部的Borg系统的特征开发的一个版本，集成了Borg系统大部分优势；

官方地址：https://Kubernetes.io

代码托管平台：https://github.com/Kubernetes



#### kubernetes具备的功能

- 自我修复：k8s可以监控容器的运行状况，并在发现容器出现异常时自动重启故障实例；
- 弹性伸缩：k8s可以根据资源的使用情况自动地调整容器的副本数。例如，在高峰时段，k8s可以自动增加容器的副本数以应对更多的流量；而在低峰时段，k8s可以减少应用的副本数，节省资源；
- 资源限额：k8s允许指定每个容器所需的CPU和内存资源，能够更好的管理容器的资源使用量；
- 滚动升级：k8s可以在不中断服务的情况下滚动升级应用版本，确保在整个过程中仍有足够的实例在提供服务；
- 负载均衡：k8s可以根据应用的负载情况自动分配流量，确保各个实例之间的负载均衡，避免某些实例过载导致的性能下降；
- 服务发现：k8s可以自动发现应用的实例，并为它们分配一个统一的访问地址。这样，用户只需要知道这个统一的地址，就可以访问到应用的任意实例，而无需关心具体的实例信息；
- 存储管理：k8s可以自动管理应用的存储资源，为应用提供持久化的数据存储。这样，在应用实例发生变化时，用户数据仍能保持一致，确保数据的持久性；
- 密钥与配置管理：Kubernetes 允许你存储和管理敏感信息，例如：密码、令牌、证书、ssh密钥等信息进行统一管理，并共享给多个容器复用；



#### kubernetes集群角色

k8s集群需要建⽴在多个节点上，将多个节点组建成一个集群，然后进⾏统⼀管理，但是在k8s集群内部，这些节点⼜被划分成了两类⻆⾊：

- 一类⻆⾊为主节点，叫Master，负责集群的所有管理工作，和协调集群中运行的容器应用； 

- ⼀类⻆⾊为⼯作节点，叫Node，负责运行集群中所有用户的容器应用， 执行实际的工作负载 ； 



**Master管理节点组件：**

- API Server：作为集群的控制中心，处理外部和内部通信，接收用户请求并处理集群内部组件之间的通信；
- Scheduler：负责将待部署的 Pods 分配到合适的 Node 节点上，根据资源需求、策略和约束等因素进行调度；
- Controller Manager：管理集群中的各种控制器，例如 Deployment、ReplicaSet、Node 控制器等，管理集群中的各种资源；
- etcd：作为集群的数据存储，保存集群的配置信息和状态信息；



**Node工作节点组件：**

- Kubelet：负责与 Master 节点通信，并根据 Master 节点的调度决策来创建、更新和删除 Pod，同时维护 Node 节点上的容器状态；
- 容器运行时（如 Docker、containerd 等）：负责运行和管理容器，提供容器生命周期管理功能。例如：创建、更新、删除容器等；
- Kube-proxy：负责为集群内的服务实现网络代理和负载均衡，确保服务的访问性；



**非必须的集群插件：**

- DNS服务：严格意义上的必须插件，在k8s中，很多功能都需要用到DNS服务，例如：服务发现、负载均衡、有状态应用的访问等；
- Dashboard： 是k8s集群的Web管理界面；
- 资源监控：例如metrics-server监视器，用于监控集群中资源利用率；

 

#### kubernetes集群类型

- 一主多从集群：由一台Master管理节点和多台Node工作节点组成，生产环境下Master节点存在单点故障的风险，适合学习和测试环境使用；

- 多主多从集群：由多台Master管理节点和多Node工作节点组成，安全性高，适合生产环境使用；



#### kubernetes集群规划

| 主机IP        | 主机名       | 主机配置 | 角色        |
| ------------- | ------------ | -------- | ----------- |
| 192.168.0.10  | k8s-master01 | 2C/4G    | 管理节点    |
| 192.168.0.11  | k8s-master02 | 2C/4G    | 管理节点    |
| 192.168.0.12  | k8s-master03 | 2C/4G    | 管理节点    |
| 192.168.0.13  | k8s-node01   | 2C/4G    | 工作节点    |
| 192.168.0.70  | k8s-ha1      | 1C/2G    | LB          |
| 192.168.0.71  | k8s-ha2      | 1C/2G    | LB          |
| 192.168.0.100 | /            | /        | VIP(虚拟IP) |



#### kubernetes集群网络

| 网络名称    | 网段           | 备注 |
| ----------- | -------------- | ---- |
| Service网络 | 10.96.0.0/16   |      |
| Pod网络     | 172.16.85.0/16 |      |



#### kubernetes集群环境部署

按照集群规划修改每个节点主机名

~~~powershell
hostnamectl set-hostname xxx
~~~



**以下前期环境准备需要在所有节点都执行** 

配置集群之间本地解析

~~~powershell
cat >> /etc/hosts << EOF
192.168.0.10 k8s-master01
192.168.0.11 k8s-master02
192.168.0.12 k8s-master03
192.168.0.13 k8s-node01
192.168.0.70 k8s-ha1
192.168.0.71 k8s-ha2
EOF
~~~



**开启bridge网桥过滤功能**

bridge(桥接) 是 Linux 系统中的一种虚拟网络设备，它充当一个虚拟的交换机，为集群内的容器提供网络通信功能，容器就可以通过这个 bridge 与其他容器或外部网络通信了。

~~~powershell
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

参数解释
net.bridge.bridge-nf-call-ip6tables = 1  //对网桥上的IPv6数据包通过iptables处理
net.bridge.bridge-nf-call-iptables = 1   //对网桥上的IPv4数据包通过iptables处理
net.ipv4.ip_forward = 1       //开启IPv4路由转发,来实现集群中的容器与外部网络的通信
~~~

~~~powershell
#由于开启bridge功能，需要加载br_netfilter模块来允许在bridge设备上的数据包经过iptables防火墙处理
modprobe br_netfilter && lsmod | grep br_netfilter

#...会输出以下内容
br_netfilter           22256  0
bridge                151336  1 br_netfilter

参数解释：
modprobe        //命令可以加载内核模块
br_netfilter    //模块模块允许在bridge设备上的数据包经过iptables防火墙处理
~~~

~~~powershell
#加载配置文件，使上述配置生效
sysctl -p /etc/sysctl.d/k8s.conf
~~~



**配置ipvs功能**

在k8s中Service有两种代理模式，一种是基于iptables的，一种是基于ipvs，两者对比ipvs负载均衡算法更加的灵活，且带有健康检查的功能，如果想要使用ipvs模式，需要手动载入ipvs模块。

`ipset` 和 `ipvsadm`  是两个与网络管理和负载均衡相关的软件包，在k8s代理模式中，提供多种负载均衡算法，如轮询（Round Robin）、最小连接（Least Connection）和加权最小连接（Weighted Least Connection）等；

~~~powershell
yum -y install ipset ipvsadm
~~~



将需要加载的ipvs相关模块写入到文件中

```powershell
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF


模块介绍
ip_vs         //提供负载均衡的模块,支持多种负载均衡算法,如轮询、最小连接、加权最小连接等
ip_vs_rr      //轮询算法的模块（默认算法）
ip_vs_wrr     //加权轮询算法的模块,根据后端服务器的权重值转发请求
ip_vs_sh      //哈希算法的模块,同一客户端的请求始终被分发到相同的后端服务器,保证会话一致性
nf_conntrack  //链接跟踪的模块,用于跟踪一个连接的状态,例如 TCP 握手、数据传输和连接关闭等
```



执行文件来加载模块

~~~powershell
chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack
~~~



**关闭SWAP分区**

为了保证 kubelet 正常工作，k8s强制要求禁用

```sh
#临时关闭
swapoff -a

#永久关闭
sed -ri 's/.*swap.*/#&/' /etc/fstab
grep "swap" /etc/fstab
```



检查swap

```sh
free -h
...
Swap:            0B          0B          0B
```



#### kubernetes负载均衡部署

此处的haproxy为apiserver提供反向代理，集群的管理请求通过VIP进行接收，haproxy将所有管理请求轮询转发到每个master节点上。

Keepalived为haproxy提供vip（192.168.0.100）在二个haproxy实例之间提供主备，降低当其中一个haproxy失效时对服务的影响。



![](二进制部署k8s1.23.0高可用集群(docker).accets/17523a485b9e77fef907090500c74a5.jpg)



**提示：以下操作只需要在[k8s-ha1]()、[k8s-ha2]()配置。**

~~~shell
yum -y install haproxy keepalived
~~~



**haproxy配置文件内容如下**

该配置文件内容在k8s-ha1与k8s-ha2节点保持一致

~~~powershell
cat /etc/haproxy/haproxy.cfg

#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
  maxconn  2000                   #单个进程最大并发连接数
  ulimit-n  16384                 #每个进程可以打开的文件数量
  log  127.0.0.1 local0 err       #日志输出配置，所有日志都记录在本机系统日志，通过 local0 输出
  stats timeout 30s               #连接socket超时时间

defaults
  log global                      #定义日志为global（全局）
  mode  http                      #使用的连接协议
  option  httplog                 #日志记录选项，httplog表示记录与HTTP会话相关的日志
  timeout connect 5000            #定义haproxy将客户端请求转发至后端服务器所等待的超时时长
  timeout client  50000           #客户端非活动状态的超时时长
  timeout server  50000           #客户端与服务器端建立连接后，等待服务器端的超时时长
  timeout http-request 15s        #客户端建立连接但不请求数据时，关闭客户端连接超时时间
  timeout http-keep-alive 15s     # session 会话保持超时时间

frontend monitor-in               #监控haproxy服务本身
  bind *:33305                    #监听的端口
  mode http                       #使用的连接协议
  option httplog                  #日志记录选项，httplog表示记录与HTTP会话相关的日志
  monitor-uri /monitor            #监控URL路径

frontend k8s-master               #接收请求的前端名称，名称自定义，类似于Nginx的一个虚拟主机server。
  bind 0.0.0.0:6443              #监听客户端请求的 IP地址和端口（以包含虚拟IP）
  bind 127.0.0.1:6443 
  mode tcp                        #使用的连接协议
  option tcplog                   #日志记录选项，tcplog表示记录与tcp会话相关的日志
  tcp-request inspect-delay 5s    #等待数据传输的最大超时时间
  default_backend k8s-master      #将监听到的客户端请求转发到指定的后端

backend k8s-master                #后端服务器组，要与前端中设置的后端名称一致
  mode tcp                        #使用的连接协议
  option tcplog                   #日志记录选项，tcplog表示记录与tcp会话相关的日志
  option tcp-check                #tcp健康检查
  balance roundrobin              #负载均衡方式为轮询
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server master01   192.168.0.10:6443  check  # 根据自己环境修改后端实例IP
  server master02   192.168.0.11:6443  check  # 根据自己环境修改后端实例IP
  server master03   192.168.0.12:6443  check  # 根据自己环境修改后端实例IP
~~~



k8s-ha1与k8s-ha2启动haproxy

~~~powershell
systemctl start haproxy
systemctl enable haproxy
systemctl status haproxy
~~~



**k8s-ha1节点keepalived配置文件内容如下**

~~~powershell
cat /etc/keepalived/keepalived.conf

! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2
rise 1
}
vrrp_instance VI_1 {
    state MASTER
    interface ens32
    virtual_router_id 51
    priority 101
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass abc123
    }
    virtual_ipaddress {
        192.168.0.100/24
    }
    track_script {
       chk_apiserver
    }
}
~~~



配置文件详解

```sh
#定义一个自定义脚本，名称为chk_apiserver
vrrp_script chk_apiserver {
    #脚本所在的路径及名称
    script "/etc/keepalived/check_apiserver.sh"
    #监控检查的时间间隔，单位秒
    interval 5
    #健康检车的次数，连续2次健康检查失败，服务器将被标记为不健康
    fall 2
    #连续健康检查成功的次数，有1次健康检查成功，服务器将被标记为健康
    rise 1
}

#配置了一个名为VI_1的VRRP实例组
vrrp_instance VI_1 {
    #该节点在VRRP组中的身份，Master节点负责处理请求并拥有虚拟IP地址
    state MASTER
    #实例绑定的网络接口，实例通过这个网络接口与其他VRRP节点通信，以及虚拟IP地址的绑定
    interface ens32
    #虚拟的路由ID，范围1到255之间的整数，用于在一个网络中区分不同的VRRP实例组，但是在同一个VRRP组中的节点，该ID要保持一致
    virtual_router_id 51
    #实例的优先级，范围1到254之间的整数，用于决定在同一个VRRP组中哪个节点将成为Master节点，数字越大优先级越>高
    priority 101
    #Master节点广播VRRP报文的时间间隔，用于通知其他Backup节点Master节点的存在和状态，在同一个VRRP组中，所有>节点的advert_int参数值必须相同
    advert_int 2
    #实例之间通信的身份验证机制
    authentication {
        #PASS为密码验证
        auth_type PASS
	#此密码必须为1到8个字符，在同一个VRRP组中，所有节点必须使用相同的密码，以确保正确的身份验证和通信
        auth_pass abc123
    }
	
    #定义虚拟IP地址
    virtual_ipaddress {
        192.168.0.100/24      
    }

    #引用自定义脚本，名称与上方vrrp_script中定义的名称保持一致
    track_script {
       chk_apiserver
    }
}
```



k8s-ha1定义检测haproxy脚本

~~~powershell
cat  /etc/keepalived/check_apiserver.sh

#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
fi
~~~



脚本添加执行权限

```sh
chmod +x /etc/keepalived/check_apiserver.sh
```



**k8s-ha2节点keepalived配置文件内容如下**

~~~powershell
cat /etc/keepalived/keepalived.conf

! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2
rise 1
}
vrrp_instance VI_1 {
    state BACKUP		#需要修改节点身份
    interface ens32
    virtual_router_id 51
    priority 99			#备用节点优先级不能高于master
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass abc123
    }
    virtual_ipaddress {
        192.168.0.100/24
    }
    track_script {
       chk_apiserver
    }
}
~~~



k8s-ha2定义检测haproxy脚本

~~~powershell
cat /etc/keepalived/check_apiserver.sh
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
~~~



脚本添加执行权限

```sh
chmod +x /etc/keepalived/check_apiserver.sh
```



k8s-ha1与k8s-ha2节点启动keepalived

~~~powershell
systemctl start keepalived
systemctl enable keepalived
systemctl status keepalived
~~~



查看集群VIP地址

~~~powershell
查看VIP（提示：ifconfig命令查看不到VIP）
[root@k8s-ha1 ~]# ip a s ens32
~~~



**kubernetes集群节点配置免密登录**

在k8s-master01上生成密钥对即可

~~~powershell
ssh-keygen
~~~



下发公钥到集群其他节点

~~~powershell
ssh-copy-id root@k8s-master02
ssh-copy-id root@k8s-master03
ssh-copy-id root@k8s-node1
~~~





# 第二章：准备集群所需证书

在k8s中，各个组件（如kubelet、kube-scheduler、kube-controller-manager等）以及用户需要进行相互通信，证书用来验证双方的身份，以确保通信的安全性。而且 k8s API 是通过HTTPS提供服务，这也需要一个 TLS（ Transport Layer Security 传输层安全协议）证书保障在客户端（如kubectl）和API服务器之间建立一个安全连接。 

> k8s集群内部通讯并不需要购买商业证书，基本上都是使用自签证书来实现集群内部的通信



创建目录用于准备集群相关的证书

> 在k8s-master01上操作即可

~~~powershell
mkdir /root/work
cd /root/work
~~~



获取 CFSSL 工具自签集群所需的证书，CFSSL 工具用于生成证书颁发机构（CA）证书和私钥、生成自签名证书、生成证书签名请求（CSR）、签发证书等。

~~~powershell
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

工具说明：
cfssl           //管理证书的命令行工具
cfssljson       //当你使用cfssl工具生成新的证书和私钥时，证书的格式是JSON对象，该工具来解析这个JSON对象，将证书和私钥提取出来，分别保存为.pem文件。
cfssl-certinfo  //这个工具可以读取PEM格式的证书，解析证书中的信息，然后将这些信息以JSON格式显示出来。
~~~



添加执行权限并将程序移动至 `/usr/local/bin/` 

~~~powershell
chmod +x cfssl*
mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
~~~



#### 生成证书签名请求

证书签名请求（Certificate Signing Request，CSR）是一种用于申请新数字证书的标准请求，当你想要从一个公共的或私有的CA （Certificate Authority，证书颁发机构） 获取一个新的证书时，证书签名请求文件中包含了要在证书中定义的一些信息，如请求者的名字、组织和地理位置等。 

~~~powershell
cat > ca-csr.json <<"EOF"
{
  "CN": "kubernetes",
  "key": {
      "algo": "rsa",
      "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "kubemsb",
      "OU": "CN"
    }
  ],
  "ca": {
          "expiry": "87600h"
  }
}
EOF


参数说明：
CN		//证书的主题名称,例如，网站证书的CN通常是其域名(自签的证书，该名称自定义)
key		//请求者的公钥,algo生成公钥的加密算法,size公钥的长度
names	//这是一个列表,包含了C(国家)、ST(省份)、L(城市)、O(公司)、OU(部门)
ca		//该证书将被标记为CA证书(可以用来签名其他证书),expiry定义新的CA证书的有效期(单位是小时)
~~~



#### 生成自签CA根证书

基于`ca-csr.json` 证书签名请求文件生成一个新的自签名的根证书（也称为CA证书，Certificate Authority证书）和与之配套的私钥。 

~~~powershell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca

命令说明：
cfssl gencert 		//用于生成新的证书和私钥
-initca				//该选项用来指定使用新的证书和私钥来签名一个证书签名请求（CSR），而不是使用现有的CA证书
cfssljson -bare ca	//将证书和私钥提取出来，分别保存为.pem文件
~~~

生成自签CA根证书后，会生成如下证书与之配套的文件：

- `ca.pem`              //这是生成的自签名证书文件,包含证书中定义的一些信息

- `ca.csr`              //这是ca证书配套的签名请求文件, 其中包含证书一些信息 

- `ca-key.pem`     //这是ca证书配套的私钥文件, 验证这个证书的真实性 



#### 配置CA根证书策略

在使用 CFSSL 工具生成自签名证书时，`ca-config.json` 文件是一个配置文件，用于定义证书颁发机构（CA）的策略和默认设置。 

~~~powershell
cat > ca-config.json <<"EOF"
{
  "signing": {
      "default": {
          "expiry": "87600h"
        },
      "profiles": {
          "kubernetes": {
              "usages": [
                  "signing",
                  "key encipherment",
                  "server auth",
                  "client auth"
              ],
              "expiry": "87600h"
          }
      }
  }
}
EOF


参数说明：
signing		//定义CA的签名配置,default表示默认配置,expiry定义了证书有效期
profiles	//定义证书策略,策略名为（kubernetes）,在该策略中通过（usages）定义了新生成的证书可以用于哪些用途,如（signing）新生成的证书可以用于数字签名、（key encipherment）密钥加密、（server auth）服务器身份认证、（client auth）和客户端身份认证
~~~





# 第三章：部署ETCD集群

k8s使用etcd作为其后端存储，以存储集群中的所有数据，例如：节点信息、Pod信息、配置资源、密钥资源、服务状态等， 满足Kubernetes对于数据存储的需求。

etcd使用Raft一致性算法保证所有的etcd实例存储的数据一致性，任何一个节点的故障都不会导致数据丢失。 



#### 生成etcd相关证书

生成etcd证书签名请求文件

~~~powershell
cat > etcd-csr.json <<"EOF"
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.0.10",
    "192.168.0.11",
    "192.168.0.12"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [{
    "C": "CN",
    "ST": "Beijing",
    "L": "Beijing",
    "O": "kubemsb",
    "OU": "CN"
  }]
}
EOF


参数说明：
CN		//证书的主题名称
hosts   //定义可以使用此证书的主机IP地址
~~~



使用现有的CA证书及其私钥 ，对`etcd-csr.json` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
-config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd


命令说明：
-ca=ca.pem 				//参数指定了用于签名新证书的 CA 证书文件
-ca-key=ca-key.pem 		//参数指定了用于签名新证书的 CA 的私钥文件
-config=ca-config.json 	//参数指定了证书的配置文件，它定义了 CA 的策略和默认设置
-profile=kubernetes		//定义证书策略,策略名为（kubernetes）
etcd-csr.json			//证书签名请求文件
cfssljson -bare ca		//将证书和私钥提取出来，分别保存为.pem文件
~~~

会生成如下证书与之配套的文件：

- `etcd.csr`				 //证书配套的签名请求文件, 包含证书中定义的一些信息
- `etcd-key.pem`        //证书配套的私钥文件, 验证证书的真实性 

- `etcd.pem`                //证书文件



#### 安装并配置etcd集群

下载etcd二进制包，GitHub下载地址：https://github.com/etcd-io/etcd/releases

~~~powershell
wget https://github.com/etcd-io/etcd/releases/download/v3.5.2/etcd-v3.5.2-linux-amd64.tar.gz
~~~



解压etcd软件包，并拷贝etcd相关工具到`/usr/local/bin/` 

~~~powershell
tar -xvf etcd-v3.5.2-linux-amd64.tar.gz
cp -p etcd-v3.5.2-linux-amd64/etcd* /usr/local/bin/
~~~



分发etcd工具到其他etcd节点

~~~shell
for etcd in k8s-master02 k8s-master03
do
	scp etcd-v3.5.2-linux-amd64/etcd* $etcd:/usr/local/bin/
done
~~~



创建etcd目录，用于存储etcd配置文件，该配置文件基于etcd内部的变量定义了etcd的相关配置

~~~powershell
mkdir /etc/etcd
cat >  /etc/etcd/etcd.conf <<"EOF"
#[Member]
ETCD_NAME="etcd1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.10:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.10:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.10:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.10:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.10:2380,etcd2=https://192.168.0.11:2380,etcd3=https://192.168.0.12:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF


变量说明：
ETCD_NAME							//当前etcd实例的名称,集群中唯一
ETCD_DATA_DIR						//etcd存储数据的目录,所有数据都将存到这个目录
ETCD_LISTEN_PEER_URLS				//etcd实例用于监听其他etcd实例连接的URL
ETCD_LISTEN_CLIENT_URLS				//etcd实例用于监听客户端连接的URL 
ETCD_INITIAL_ADVERTISE_PEER_URLS	//etcd实例用于向集群中的其他实例宣告自己的URL
ETCD_ADVERTISE_CLIENT_URLS			//etcd实例用于向客户端宣告自己的URL
ETCD_INITIAL_CLUSTER				//初始化集群时所有etcd实例的列表
ETCD_INITIAL_CLUSTER_TOKEN			//集群Token,用于标识集群的唯一标记
ETCD_INITIAL_CLUSTER_STATE			//etcd集群的初始状态，new是新集群，existing表示加入已有集群
~~~



创建目录存放etcd证书

~~~powershell
mkdir /etc/etcd/ssl
~~~



创建etcd数据目录

```sh
mkdir -p /var/lib/etcd/default.etcd
```



拷贝相关证书到etcd证书目录

~~~powershell
cd /root/work
cp ca*.pem /etc/etcd/ssl
cp etcd*.pem /etc/etcd/ssl
~~~



为etcd创建服务管理文件 (systemd管理etcd)

~~~powershell
cat > /etc/systemd/system/etcd.service <<"EOF"
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=-/etc/etcd/etcd.conf
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/local/bin/etcd \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF


[Service] 部分参数介绍：
Type				//服务启动类型,"notify"表示服务启动完成后向Systemd发送一个通知
EnvironmentFile		//指定etcd环境变量文件,服务会在启动时加载文件中的环境变量
WorkingDirectory	//定义了服务的工作目录,此处是 /var/lib/etcd/
ExecStart 			//定义了启动服务时要执行的命令,通过etcd的选项指定了相关证书
Restart 			//定义了服务失败时应如何操作,"on-failure"意味着在服务失败时重启服务
RestartSec			//定义了服务失败后等待多久才重启服务,此处是 5 秒
LimitNOFILE			//设置服务进程能打开的最大文件数,此处是 65536
~~~



同步文件到集群其它 (etcd) 节点，提前在其他节点创建好对应的目录

~~~powershell
mkdir -p /etc/etcd
mkdir -p /etc/etcd/ssl
mkdir -p /var/lib/etcd/default.etcd
~~~



同步文件

~~~powershell
for etcd in k8s-master02 k8s-master03
do
scp /etc/etcd/etcd.conf $etcd:/etc/etcd
scp /etc/etcd/ssl/* $etcd:/etc/etcd/ssl
scp /etc/systemd/system/etcd.service $etcd:/etc/systemd/system
done
~~~



**etcd配置文件需要修改etcd节点名称及IP地址**

k8s-master02节点

~~~powershell
cat /etc/etcd/etcd.conf
#[Member]
ETCD_NAME="etcd2"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.11:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.11:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.11:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.11:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.10:2380,etcd2=https://192.168.0.11:2380,etcd3=https://192.168.0.12:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
~~~



k8s-master03节点

~~~powershell
cat /etc/etcd/etcd.conf
#[Member]
ETCD_NAME="etcd3"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.12:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.12:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.12:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.12:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.10:2380,etcd2=https://192.168.0.11:2380,etcd3=https://192.168.0.12:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
~~~



#### 启动etcd集群并验证

~~~powershell
systemctl daemon-reload
systemctl start etcd
systemctl enable etcd
systemctl status etcd
~~~



验证集群状态

~~~powershell
ETCDCTL_API=3 /usr/local/bin/etcdctl --write-out=table --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.0.10:2379,https://192.168.0.11:2379,https://192.168.0.12:2379 endpoint health

会有如下输出：
+---------------------------+--------+-------------+-------+
|         ENDPOINT          | HEALTH |    TOOK     | ERROR |
+---------------------------+--------+-------------+-------+
| https://192.168.0.10:2379 |   true | 11.442359ms |       |
| https://192.168.0.12:2379 |   true | 11.504115ms |       |
| https://192.168.0.11:2379 |   true | 11.335585ms |       |
+---------------------------+--------+-------------+-------+
~~~





# 第四章：Kubernetes集群部署



### 准备docker环境

准备阿里云docker仓库（集群所有节点，不包括LB节点）

~~~powershell
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
~~~



下载docker软件

~~~powershell
yum -y install docker-ce-20.10.9-3.el7.x86_64
~~~



启动docker服务

~~~powershell
systemctl enable docker
systemctl start docker
~~~



设置docker的Cgroup用于对容器实现资源限制

~~~powershell
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
~~~



重启docker服务

~~~powershell
systemctl daemon-reload
systemctl start docker
~~~



### kubernetes集群软件下载

下载地址：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md



![](二进制部署k8s1.23.0高可用集群(docker).accets/1683898930534.png)



**在k8s-master01节点下载即可**

~~~powershell
wget https://dl.k8s.io/v1.23.0/kubernetes-server-linux-amd64.tar.gz
~~~



解压软件包并拷贝集群组件到`/usr/local/bin`

~~~powershell
tar -xvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes/server/bin/
cp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl /usr/local/bin
~~~



拷贝master组件到k8s-master02、k8s-master03的`/usr/local/bin`

~~~shell
for master in k8s-master02 k8s-master03
do
	scp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl /usr/local/bin $master:/usr/local/bin
done
~~~



拷贝node组件到k8s-node01的`/usr/local/bin`

~~~shell
for node in k8s-node01
do
	scp kubelet kube-proxy $node:/usr/local/bin
done
~~~



在集群所有(不包括LB节点)节点上创建目录

~~~powershell
#用于存储集群配置文件
mkdir -p /etc/kubernetes/        

#用于存储集群证书文件
mkdir -p /etc/kubernetes/ssl     

#用于存储集群日志文件
mkdir -p /var/log/kubernetes 
~~~



### 部署kube-apiserver组件

##### 生成apiserver相关证书

生成apiserver证书签名请求文件

~~~powershell
cat > kube-apiserver-csr.json << "EOF"
{
"CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.0.10",
    "192.168.0.11",
    "192.168.0.12",
    "192.168.0.13",
    "192.168.0.14",
    "192.168.0.15",
    "192.168.0.16",
    "192.168.0.17",
    "192.168.0.100",
    "10.96.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "kubemsb",
      "OU": "CN"
    }
  ]
}
EOF


参数说明：
CN		//证书的主题名称
hosts   //定义可以使用此证书的主机IP（含VIP）地址,方便后期扩容可以多预留一些IP,10.96.0.1是 k8s服务发现机制使用的IP
"kubernetes"、"kubernetes.default"、"kubernetes.default.svc"、"kubernetes.default.svc.cluster" 、"kubernetes.default.svc.cluster.local" 是k8s集群中用于访问 API 服务器的服务名和全域名,对这些名字的请求也将被视为有效。
~~~



使用现有的CA证书及其私钥 ，对`kube-apiserver-csr.json` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
~~~

会生成如下证书与之配套的文件：

- `kube-apiserver.csr`				 //证书配套的签名请求文件, 包含证书中定义的一些信息
- `kube-apiserver-key.pem`        //证书配套的私钥文件, 验证证书的真实性 

- `kube-apiserver.pem`                //证书文件



创建一个token（令牌）文件，k8s引入了TLS bootstraping机制来自动颁发客户端证书，当Node节点很多时，来简化证书颁发流程（在kubelet章节在详细讲解）

~~~powershell
cat > token.csv << EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF


#格式：token，用户名，UID，用户组
~~~



##### 创建apiserver服务配置文件

该配置文件基于apiserver内部的变量定义了相关配置

~~~powershell
cat > /etc/kubernetes/kube-apiserver.conf << "EOF"
KUBE_APISERVER_OPTS="--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --anonymous-auth=false \
  --bind-address=192.168.0.10 \
  --secure-port=6443 \
  --advertise-address=192.168.0.10 \
  --insecure-port=0 \
  --authorization-mode=Node,RBAC \
  --runtime-config=api/all=true \
  --enable-bootstrap-token-auth \
  --service-cluster-ip-range=10.96.0.0/16 \
  --token-auth-file=/etc/kubernetes/token.csv \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \
  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \
  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \
  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \
  --service-account-issuer=api \
  --etcd-cafile=/etc/etcd/ssl/ca.pem \
  --etcd-certfile=/etc/etcd/ssl/etcd.pem \
  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \
  --etcd-servers=https://192.168.0.10:2379,https://192.168.0.11:2379,https://192.168.0.12:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/kube-apiserver-audit.log \
  --event-ttl=1h \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=4"
EOF


插件说明：
--enable-admission-plugins=	//启用一组准入控制插件,在API请求被永久写入存储之前进行检查
NamespaceLifecycle			//该插件会拒绝那些正在终止的命名空间中的请求
NodeRestriction				//该插件限制kubelet只能管理自己的节点的Pod
LimitRanger					//该插件实现资源使用的默认限制和配额   
ServiceAccount				//该插件则确保在创建的每个Pod中都有一个ServiceAccount
DefaultStorageClass			//该插件实现动态存储(StorageClass)的PVC绑定
ResourceQuota				//该插件实现对命名空间的资源使用量设定配额

选项说明：
--anonymous-auth=false			//禁止匿名用户访问API服务器
--bind-address					//API服务器监听的IP地址
--secure-port					//API服务器监听HTTPS连接的端口
--advertise-address				//这是API服务器公告给集群中其它组件的IP地址
--insecure-port					//此选项设置为0,表示API服务器监听不安全的HTTP连接
--authorization-mode			//这是API服务器使用的授权模式
--runtime-config				//这个选项用于启用或禁用API版本或特定的API资源
--enable-bootstrap-token-auth	//启用用于节点引导的令牌认证
--service-cluster-ip-range		//为Service分配的IP地址范围
--token-auth-file				//此文件用于通过令牌进行身份验证
--service-node-port-range		//为NodePort Service分配的端口范围
--service-account-issuer		//ServiceAccount令牌的颁发者
--enable-swagger-ui				//启用可视化功能,可通过浏览器向你的API发送请求
--allow-privileged				//是否允许创建具有特权的容器
--apiserver-count				//用于端点选举的API服务器的数量
--audit-log-maxage、--audit-log-maxbackup、--audit-log-maxsize、--audit-log-path								 //这些选项配置了审计日志功能
--alsologtostderr=true			//将日志输出到标准错误（stderr）避免日志丢失
--logtostderr=false				//生成日志文件
--log-dir=/var/log/kubernetes	//指定kube-apiserver将日志写入到的目录
--v=4							//日志等级,数值越大,输出的日志越详细
~~~



##### 创建apiserver服务管理文件

systemd管理apiserver

~~~powershell
cat > /etc/systemd/system/kube-apiserver.service << "EOF"
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=etcd.service
Wants=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/kube-apiserver.conf
ExecStart=/usr/local/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF


[Unit] 部分参数说明：
Description			//简单的服务描述
Documentation		//服务文档的链接
After				//这个服务会在etcd.service启动后启动
Wants				//如果etcd.service服务失败，这个服务仍会尝试启动

[Service] 部分参数说明：
EnvironmentFile		//指向服务的配置文件
ExecStart			//定义了启动服务时运行的命令,基于$KUBE_APISERVER_OPTS变量中选项运行
Restart				//如果服务进程失败（退出状态非零）systemd将重新启动它
RestartSec			//定义了在服务重启之间的延迟,如果服务需要重启,systemd将等待5秒
Type				//定义了服务的类型,notify表示服务在准备好接受请求时给systemd发送通知
LimitNOFILE			//定义了服务进程可以打开的最大文件数量
~~~



##### 同步文件到集群master节点

同步本机

~~~powershell
cp ca*.pem /etc/kubernetes/ssl/
cp kube-apiserver*.pem /etc/kubernetes/ssl/
cp token.csv /etc/kubernetes/
~~~



其他master节点

~~~powershell
for master in k8s-master02 k8s-master03
do
	scp /etc/kubernetes/kube-apiserver.conf $master:/etc/kubernetes
	scp /etc/kubernetes/ssl/ca*.pem $master:/etc/kubernetes/ssl
	scp /etc/kubernetes/ssl/kube-apiserver*.pem $master:/etc/kubernetes/ssl
	scp /etc/kubernetes/token.csv $master:/etc/kubernetes
	scp /etc/systemd/system/kube-apiserver.service $master:/etc/systemd/system
done
~~~



k8s-master02需要修改文件中的`--bind-address 与--advertise-address` IP为本机

~~~powershell
[root@k8s-master02 ~]# egrep "bind-addres|advertise-address" /etc/kubernetes/kube-apiserver.conf
  --bind-address=192.168.0.11 \
  --advertise-address=192.168.0.11 \
~~~



k8s-master03需要修改文件中的`--bind-address 与--advertise-address` IP为本机

~~~powershell
[root@k8s-master03 ~]# egrep "bind-addres|advertise-address" /etc/kubernetes/kube-apiserver.conf
  --bind-address=192.168.0.12 \
  --advertise-address=192.168.0.12 \
~~~



##### 启动apiserver服务

~~~powershell
systemctl daemon-reload
systemctl start kube-apiserver
systemctl enable kube-apiserver
systemctl status kube-apiserver

# 测试
curl --insecure https://192.168.0.10:6443/
curl --insecure https://192.168.0.11:6443/
curl --insecure https://192.168.0.12:6443/
curl --insecure https://192.168.0.100:6443/
~~~



### 部署kubectl组件

##### 生成kubectl相关证书

生成kubectl证书签名请求文件

~~~powershell
cat > admin-csr.json << "EOF"
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",             
      "OU": "system"
    }
  ]
}
EOF


说明：这个admin证书，是将来生成管理员用的kube.config配置文件用的,“O”（公司）参数必须是system:masters，否则后面kubectl create clusterrolebinding报错
~~~



使用现有的CA证书及其私钥 ，对`admin-csr.json` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
~~~

会生成如下证书与之配套的文件：

- `admin.csr `				 //证书配套的签名请求文件, 包含证书中定义的一些信息
- `admin-key.pem`        //证书配套的私钥文件, 验证证书的真实性 

- `admin.pem`                //证书文件



复制证书文件到`/etc/kubernetes/ssl`

~~~powershell
cp admin*.pem /etc/kubernetes/ssl
~~~



##### 配置kubectl安全上下文

`kube.config` 是 `kubectl` 的安全上下文，该文件用于存储集群，用户，命名空间和认证信息的文件，kubectl 命令行工具用它来管理集群

~~~powershell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube.config


命令说明：
kubectl config set-cluster 			//设置集群，kubernetes是集群名称
--certificate-authority=ca.pem		//验证API server的证书文件
--embed-certs=true					//将证书数据嵌入到kube.config文件中,这样即使证书文件移动或删除，kube.config文件仍然有效
--server=https://192.168.0.100:6443	//这个参数指定了API server的地址（VIP地址）
~~~



在 `kube.config` 文件中设置集群管理员（ `admin`）用户

```powershell
kubectl config set-credentials admin --client-certificate=admin.pem --client-key=admin-key.pem --embed-certs=true --kubeconfig=kube.config


命令说明：
kubectl config set-credentials	//设置集群用户，用户名admin
--client-certificate=admin.pem	//验证admin的证书文件
--client-key=admin-key.pem		//验证admin的私钥文件
--embed-certs=true				//将证书数据嵌入到kube.config文件中,这样即使证书文件移动或删除，kube.config文件仍然有效
--kubeconfig=kube.config		//指定了将被修改的kubeconfig文件名
```



在`kube.config`文件中设置集群的上下文（context）信息，它会将集群、用户、命名空间关联起来， 以便快速轻松地在不同的集群，用户和命名空间之间切换

```powershell
kubectl config set-context kubernetes --cluster=kubernetes --user=admin --kubeconfig=kube.config


命令说明：
kubectl config set-context 		//设置集群上下文，kubernetes是context名称
--cluster=kubernetes			//指定了context关联的集群名称
--user=admin					//指定了context关联的用户的名称（admin管理员）
--kubeconfig=kube.config		//指定了将被修改的kubeconfig文件名
```



通过`kube.config`定义的`context`切换到 kubernetes集群环境验证

```powershell
kubectl config use-context kubernetes --kubeconfig=kube.config
```



拷贝`kube.config`配置文件到用户家目录(该文件在那个用户家目录，那个用户就可以基于该文件的admin权限管理集群)

~~~powershell
mkdir /root/.kube
cp kube.config /root/.kube/config
~~~



创建集群角色绑定， 用于将集群管理员的角色 (权限) 绑定给admin用户，这样admin用户就具备集群管理员的权限

```powershell
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes --kubeconfig=/root/.kube/config
```



##### 查看集群状态

设置环境变量 `KUBECONFIG`，用于保存`config`配置文件，kubectl 工具需要这个文件来管理集群

~~~powershell
export KUBECONFIG=$HOME/.kube/config
~~~



查看集群状态

~~~powershell
#查看正在运行的集群的基本信息
kubectl cluster-info

#检查集群的各个管理节点组件的健康状况
kubectl get componentstatuses

#查看集群中的所有资源的信息（如：namespace、pods、service、deployment等）
kubectl get all --all-namespaces
~~~



##### 同步文件到集群master节点

提前在其他节点创建好对应的目录

~~~powershell
mkdir /root/.kube

for master in k8s-master02 k8s-master03
do
	scp /root/.kube/config $master:/root/.kube/config
done
~~~



### 部署kube-controller-manager

##### 生成controller-manager相关证书

生成controller-manager证书签名请求文件

~~~powershell
cat > kube-controller-manager-csr.json << "EOF"
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "hosts": [
      "127.0.0.1",
      "192.168.0.10",
      "192.168.0.11",
      "192.168.0.12"
    ],
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-controller-manager",
        "OU": "system"
      }
    ]
}
EOF


关键参数说明：
hosts		//包含所有controller-manager节点IP
O（公司） 	 //system:kube-controller-manager是集群内置的ClusterRoleBindings，赋予 kube-controller-manager工作所需的权限
~~~



使用现有的CA证书及其私钥 ，对`kube-controller-manager-csr.json` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
~~~

会生成如下证书与之配套的文件：

- `kube-controller-manager.csr `			//证书配套的签名请求文件, 包含证书中定义的一些信息
- `kube-controller-manager-key.pem `    //证书配套的私钥文件, 验证证书的真实性 

- `kube-controller-manager.pem`             //证书文件



##### 配置controller-manager安全上下文

`kube-controller-manager.kubeconfig` 是controller-manager安全上下文，该文件中用于存储集群，用户，命名空间和认证信息，controller-manager用它来找到自己的集群

~~~powershell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-controller-manager.kubeconfig
~~~



在文件中设置 `system:kube-controller-manager` 用户

```sh
kubectl config set-credentials system:kube-controller-manager --client-certificate=kube-controller-manager.pem --client-key=kube-controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig
```



在文件中设置集群的上下文（context）信息，这样`system:kube-controller-manager` 用户就可以找到自己的集群

```sh
kubectl config set-context system:kube-controller-manager --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
```



通过文件定义的`context`切换到 kubernetes集群环境验证

```sh
kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
```



##### 创建controller-manager服务配置文件

该配置文件基于controller-manager内部的变量定义了相关配置

~~~powershell
cat > kube-controller-manager.conf << "EOF"
KUBE_CONTROLLER_MANAGER_OPTS="--port=0 \
  --secure-port=10257 \
  --bind-address=127.0.0.1 \
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
  --service-cluster-ip-range=10.96.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --allocate-node-cidrs=true \
  --cluster-cidr=10.244.0.0/16 \
  --experimental-cluster-signing-duration=87600h \
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --leader-elect=true \
  --feature-gates=RotateKubeletServerCertificate=true \
  --controllers=*,bootstrapsigner,tokencleaner \
  --horizontal-pod-autoscaler-sync-period=10s \
  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
  --use-service-account-credentials=true \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2"
EOF


选项说明：
--port=0					//关闭不安全HTTP端口访问
--secure-port				//kube-controller-manager的HTTPS端口
--bind-address				//kube-controller-manager绑定的本机IP地址
--kubeconfig				//kube-controller-manager上下文,用于跟API server通信
--service-cluster-ip-range	//指定服务IP地址范围
--cluster-name				//指定当前集群的名称
--allocate-node-cidrs		//启动集群CIDR（即子网）用于Pod网络
--cluster-cidr				//集群的CIDR（即子网）范围
--experimental-cluster-signing-duration=87600h	//证书签名的持续时间
--leader-elect				//启用leader（领导者）选举，实现集群高可用性
--feature-gates=RotateKubeletServerCertificate	//启用自动管理和更新Kubelet的TLS证书
--controllers				//表示启动所有默认的控制器(*)，并额外启动bootstrapsigner 和tokencleaner控制器,主要用于身份验证
--horizontal-pod-autoscaler-sync-period	//HPA控制器规则检测时间
--use-service-account-credentials		//每个控制使用单独的服务帐户凭证进行身份验证
--alsologtostderr			//日志写入到标准错误，避免日志丢失
~~~



##### 创建controller-manager服务管理文件

~~~powershell
cat > kube-controller-manager.service << "EOF"
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-controller-manager.conf
ExecStart=/usr/local/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
~~~



##### 同步文件到集群master节点

同步本机

~~~powershell
cp kube-controller-manager*.pem /etc/kubernetes/ssl/
cp kube-controller-manager.kubeconfig /etc/kubernetes/
cp kube-controller-manager.conf /etc/kubernetes/
cp kube-controller-manager.service /usr/lib/systemd/system/
~~~



其他master节点

~~~shell
for master in k8s-master02 k8s-master03
do
	scp  kube-controller-manager*.pem $master:/etc/kubernetes/ssl/
	scp  kube-controller-manager.kubeconfig $master:/etc/kubernetes/
	scp  kube-controller-manager.conf $master:/etc/kubernetes/
	scp  kube-controller-manager.service $master:/usr/lib/systemd/system/
done
~~~



##### 启动controller-manager服务

~~~powershell
systemctl daemon-reload
systemctl start kube-controller-manager
systemctl enable kube-controller-manager
systemctl status kube-controller-manager
~~~



### 部署kube-scheduler

##### 生成scheduler相关证书

生成scheduler证书签名请求文件

~~~powershell
cat > kube-scheduler-csr.json << "EOF"
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
      "192.168.0.10",
      "192.168.0.11",
      "192.168.0.12"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-scheduler",
        "OU": "system"
      }
    ]
}
EOF


关键参数说明：
hosts		//包含所有scheduler节点IP
O（公司） 	 //system:kube-scheduler是集群内置的ClusterRoleBindings，赋予kube-scheduler工作所需的权限
~~~



使用现有的CA证书及其私钥 ，对`kube-scheduler` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
~~~

会生成如下证书与之配套的文件：

- `kube-scheduler.csr  `			//证书配套的签名请求文件, 包含证书中定义的一些信息
- `kube-scheduler-key.pem   `    //证书配套的私钥文件, 验证证书的真实性 

- `kube-scheduler.pem`             //证书文件



##### 配置scheduler安全上下文

`kube-scheduler.kubeconfig` 是scheduler安全上下文，该文件中用于存储集群，用户，命名空间和认证信息，scheduler用它来找到自己的集群

~~~powershell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-scheduler.kubeconfig
~~~



在文件中设置 `system:kube-scheduler` 用户

```sh
kubectl config set-credentials system:kube-scheduler --client-certificate=kube-scheduler.pem --client-key=kube-scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig
```



在文件中设置集群的上下文（context）信息，这样`system:kube-scheduler` 用户就可以找到自己的集群

```sh
kubectl config set-context system:kube-scheduler --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
```



通过文件定义的`context`切换到 kubernetes集群环境验证

```sh
kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
```



##### 创建scheduler服务配置文件

该配置文件基于scheduler内部的变量定义了相关配置

~~~powershell
cat > kube-scheduler.conf << "EOF"
KUBE_SCHEDULER_OPTS="--address=127.0.0.1 \
--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--leader-elect=true \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
~~~



##### 创建scheduler服务管理文件

~~~powershell
cat > kube-scheduler.service << "EOF"
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-scheduler.conf
ExecStart=/usr/local/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
~~~



##### 同步文件到集群master节点

同步本机

~~~powershell
cp kube-scheduler*.pem /etc/kubernetes/ssl/
cp kube-scheduler.kubeconfig /etc/kubernetes/
cp kube-scheduler.conf /etc/kubernetes/
cp kube-scheduler.service /usr/lib/systemd/system/
~~~



其他master节点

~~~shell
for master in k8s-master02 k8s-master03
do
	scp  kube-scheduler*.pem $master:/etc/kubernetes/ssl/
	scp  kube-scheduler.kubeconfig $master:/etc/kubernetes/
	scp  kube-scheduler.conf $master:/etc/kubernetes/
	scp  kube-scheduler.service $master:/usr/lib/systemd/system/
done
~~~



##### 启动scheduler服务

~~~powershell
systemctl daemon-reload
systemctl start kube-scheduler
systemctl enable kube-scheduler
systemctl status kube-scheduler
~~~



### 部署kubelet组件

##### 定义bootstrap.kubeconfig

kubelet在集群中负责启动和管理容器，也需要使用证书进行验证，然而，为新加入集群的节点手动颁发这些证书非常不方便，k8s引入了TLS bootstraping机制来简化证书颁发流程。

kubelet 会使用一个预先提供的 "bootstrap" 证书来向 apiserver 发送一个证书签名请求 (CSR) ，从而自动生成 kubelet 的 TLS 证书。 

```sh
BOOTSTRAP_TOKEN=$(awk -F "," '{print $1}' /etc/kubernetes/token.csv)
```



##### 配置kubelet安全上下文

`kubelet-bootstrap.kubeconfig` 作为kubelet安全上下文，该文件中用于存储集群，用户，命名空间和认证信息，scheduler用它来找到自己的集群

~~~powershell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kubelet-bootstrap.kubeconfig
~~~



在文件中设置 `system:kubelet-bootstrap` 用户

```sh
kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=kubelet-bootstrap.kubeconfig
```



在文件中设置集群的上下文（context）信息，这样`system:kubelet-bootstrap` 用户就可以找到自己的集群

```sh
kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig
```



通过文件定义的`context`切换到 kubernetes集群环境验证

```sh
kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig
```



创建集群角色绑定， 用于将集群管理员的`cluster-admin`角色 (权限) 绑定给`kubelet-bootstrap`用户，这样`kubelet-bootstrap`用户就具备集群管理员的权限

~~~powershell
kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=kubelet-bootstrap

kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig
~~~



##### 创建kubelet服务配置文件

Kubelet 的配置文件，定义了 Kubelet 运行所需的一些配置参数 

~~~powershell
cat > kubelet.json << "EOF"
{
  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
  "authentication": {
    "x509": {
      "clientCAFile": "/etc/kubernetes/ssl/ca.pem"
    },
    "webhook": {
      "enabled": true,
      "cacheTTL": "2m0s"
    },
    "anonymous": {
      "enabled": false
    }
  },
  "authorization": {
    "mode": "Webhook",
    "webhook": {
      "cacheAuthorizedTTL": "5m0s",
      "cacheUnauthorizedTTL": "30s"
    }
  },
  "address": "192.168.0.10",
  "port": 10250,
  "readOnlyPort": 10255,
  "cgroupDriver": "systemd",                    
  "hairpinMode": "promiscuous-bridge",
  "serializeImagePulls": false,
  "clusterDomain": "cluster.local.",
  "clusterDNS": ["10.96.0.2"]
}
EOF


参数说明：
kind				//说明这个文档定义的是Kubelet配置
apiVersion			//配置文件遵循的API版本
authentication		//配置Kubelet的身份验证设置,包括 x509、webhook和匿名三种方式
x509				//用于验证来自API服务器的请求的CA证书文件
webhook				//启用基于webhook的身份验证和缓存TTL
anonymous			//关闭匿名用户的访问
authorization		//配置Kubelet的授权模式为Webhook,设置授权和未授权的缓存TTL
address 			//Kubelet服务监听的IP地址
port				//Kubelet服务监听的端口号
readOnlyPort		//Kubelet只读端口号
cgroupDriver		//Kubelet使用的cgroup驱动,用于限制容器的资源使用量
hairpinMode			//同构bridge实现Pod中的不同容器间通信
serializeImagePulls	//此设置为false表示允许Kubelet并行拉取镜像
clusterDomain		//设置Kubernetes集群的DNS域名
clusterDNS			//设置集群DNS服务器的地址
~~~



##### 创建kubelet服务管理文件

~~~powershell
cat > kubelet.service << "EOF"
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --config=/etc/kubernetes/kubelet.json \
  --network-plugin=cni \
  --rotate-certificates \
  --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2 \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


参数说明：
Description					//服务的简要描述
Documentation				//服务的文档链接
After						//定义了该服务启动应该在哪些服务之后
Requires					//定义了该服务启动需要哪些服务

[Service]					//定义了服务的启动细节
WorkingDirectory			//定义了Kubelet的工作目录
ExecStart					//定义了启动服务的命令和参数
--network-plugin			//指定kubelet使用的网络插件
--rotate-certificates		//启用kubelet证书轮换,证书有效期限结束时自动请求新的证书
--pod-infra-container-image	//指定pause容器的镜像
--alsologtostderr			//日志输出到标准错误,避免日志丢失
--log-dir					//日志文件的路径
Restart						//定义了服务在失败时是否重启
RestartSec					//定义了服务失败后重启前的等待时间
~~~



##### 同步文件到集群所有节点

同步本机

~~~powershell
cp kubelet-bootstrap.kubeconfig /etc/kubernetes/
cp kubelet.json /etc/kubernetes/
cp kubelet.service /usr/lib/systemd/system/
~~~



集群所有节点

~~~shell
for k8s in  k8s-master02 k8s-master03 k8s-node01
do
	scp kubelet-bootstrap.kubeconfig kubelet.json $k8s:/etc/kubernetes/
	scp ca.pem $k8s:/etc/kubernetes/ssl/
	scp kubelet.service $k8s:/usr/lib/systemd/system/
done
~~~



提示：`kubelet.json`中`address`需要修改为当前主机IP地址。

~~~powershell
#k8s-master02节点
[root@k8s-master02 ~]# grep address /etc/kubernetes/kubelet.json
  "address": "192.168.0.11",


#k8s-master03节点
[root@k8s-master03 ~]# grep address /etc/kubernetes/kubelet.json
  "address": "192.168.0.12",
  

#k8s-node01节点
[root@k8s-node01 ~]# grep address /etc/kubernetes/kubelet.json
  "address": "192.168.0.13",
~~~



所有节点创建kubelet工作目录

~~~powershell
mkdir /var/lib/kubelet
~~~



检查所有节点是否都存在日志目录，如果不存在则创建

```sh
ls /var/log/kubernetes
```



##### 启动kubelet服务

~~~powershell
systemctl daemon-reload
systemctl start kubelet
systemctl status kubelet
systemctl enable kubelet
~~~



查看集群节点信息（在master节点执行命令）

~~~powershell
# kubectl get nodes
NAME           STATUS     ROLES    AGE     VERSION
k8s-master01   NotReady   <none>   2m35s   v1.23.0
k8s-master02   NotReady   <none>   86s     v1.23.0
k8s-master03   NotReady   <none>   55s     v1.23.0
k8s-node01     NotReady   <none>   36s     v1.23.0
~~~



### 部署kube-proxy

##### 生成kube-proxy相关证书

生成kubeproxy证书签名请求文件

~~~powershell
cat > kube-proxy-csr.json << "EOF"
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "kubemsb",
      "OU": "CN"
    }
  ]
}
EOF
~~~



使用现有的CA证书及其私钥 ，对`kube-proxy` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

~~~powershell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
~~~

会生成如下证书与之配套的文件：

- `kube-proxy.csr  `			//证书配套的签名请求文件, 包含证书中定义的一些信息
- `kube-proxy-key.pem   `    //证书配套的私钥文件, 验证证书的真实性 

- `kube-proxy.pem`             //证书文件



##### 配置kube-proxy安全上下文

`kube-proxy.kubeconfig` 是kube-proxy安全上下文，该文件中用于存储集群，用户，命名空间和认证信息，kube-proxy用它来找到自己的集群

~~~powershell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-proxy.kubeconfig
~~~



在文件中设置 `kube-proxy` 用户

```sh
kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --client-key=kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig
```



在文件中设置集群的上下文（context）信息，这样`kube-proxy` 用户就可以找到自己的集群

```sh
kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig
```



通过文件定义的`context`切换到 kubernetes集群环境验证

```sh
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```



##### 创建kube-proxy服务配置文件

Kube-proxy的配置文件，定义了 Kube-proxy 运行所需的一些配置参数 

~~~powershell
cat > kube-proxy.yaml << "EOF"
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 192.168.0.10
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 10.244.0.0/16
healthzBindAddress: 192.168.0.10:10256
kind: KubeProxyConfiguration
metricsBindAddress: 192.168.0.10:10249
mode: "ipvs"
EOF


参数说明：
apiVersion			//表明此配置遵循的API版本
bindAddress			//kube-proxy监听的IP地址,用于接收传入的请求
clusterCIDR			//集群的IP地址范围,这样kube-proxy进行适当的路由
healthzBindAddress	//kube-proxy健康检查端口,其他组件通过这个地址来检查它的状态
kind				//表明这个文件的类型是KubeProxyConfiguration
metricsBindAddress	//指定kube-proxy提供指标数据端口,在这个端口获取kube-proxy的性能指标
mode				//指定kube-proxy使用的代理模式为ipvs模式是一种基于内核的负载均衡技术
~~~



##### 创建kube-proxy服务管理文件

~~~powershell
cat >  kube-proxy.service << "EOF"
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
~~~



#####  同步文件到集群所有节点

同步本机

~~~powershell
cp kube-proxy*.pem /etc/kubernetes/ssl/
cp kube-proxy.kubeconfig kube-proxy.yaml /etc/kubernetes/
cp kube-proxy.service /usr/lib/systemd/system/
~~~



集群所有节点

~~~shell
for k8s in k8s-master02 k8s-master03 k8s-node01
do
	scp kube-proxy.kubeconfig kube-proxy.yaml $k8s:/etc/kubernetes/
	scp kube-proxy.service $k8s:/usr/lib/systemd/system/
done
~~~



提示：`kube-proxy.yaml` 文件中 `bindAddress、healthzBindAddress、metricsBindAddress` 需要修改为当前主机IP地址。

~~~powershell
#k8s-master02节点
[root@k8s-master02 ~]# egrep "bindAddress|healthzBindAddress|metricsBindAddress" /etc/kubernetes/kube-proxy.yaml
bindAddress: 192.168.0.11
healthzBindAddress: 192.168.0.11:10256
metricsBindAddress: 192.168.0.11:10249


#k8s-master03节点
[root@k8s-master03 ~]# egrep "bindAddress|healthzBindAddress|metricsBindAddress" /etc/kubernetes/kube-proxy.yaml
bindAddress: 192.168.0.12
healthzBindAddress: 192.168.0.12:10256
metricsBindAddress: 192.168.0.12:10249


#k8s-node01节点
[root@k8s-node01 ~]# egrep "bindAddress|healthzBindAddress|metricsBindAddress" /etc/kubernetes/kube-proxy.yaml
bindAddress: 192.168.0.13
healthzBindAddress: 192.168.0.13:10256
metricsBindAddress: 192.168.0.13:10249
~~~



所有节点创建kube-proxy工作目录

```sh
mkdir /var/lib/kube-proxy
```



##### 启动kube-proxy服务

~~~powershell
systemctl daemon-reload
systemctl start kube-proxy
systemctl status kube-proxy
systemctl enable kube-proxy
~~~





### 部署Calico网络

Calico 和 Flannel 是两种流行的 k8s 网络插件，它们都为集群中的 Pod 提供网络功能。然而，它们在实现方式和功能上有一些重要区别： 



**网络模型的区别：**

- Calico 使用 BGP（边界网关协议）作为其底层网络模型。它利用 BGP 为每个 Pod 分配一个唯一的 IP 地址，并在集群内部进行路由。[Calico 支持网络策略，可以对流量进行精细控制，允许或拒绝特定的通信]()。
- Flannel 则采用了一个简化的覆盖网络模型。它为每个节点分配一个 IP 地址子网，然后在这些子网之间建立覆盖网络。Flannel 将 Pod 的数据包封装到一个更大的网络数据包中，并在节点之间进行转发。[Flannel 更注重简单和易用性，不提供与 Calico 类似的网络策略功能]()。



**性能的区别：**

- 由于 Calico 使用 BGP 进行路由，其性能通常优于 Flannel。[Calico 可以实现直接的 Pod 到 Pod 通信，而无需在节点之间进行额外的封装和解封装操作]()。这使得 Calico 在大型或高度动态的集群中具有更好的性能。
- [Flannel 的覆盖网络模型会导致额外的封装和解封装开销，从而影响网络性能]()。对于较小的集群或对性能要求不高的场景，这可能并不是一个严重的问题。



在k8s-master01节点安装Calico网络即可

~~~powershell
#下载calico文件
wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml

#创建calico网络
kubectl apply -f calico.yaml 


#查看calico的Pod状态是否为Running
kubectl get pod -n kube-system

NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-66966888c4-whdkj   1/1     Running   0          101s
calico-node-f4ghp                          1/1     Running   0          101s
calico-node-sj88q                          1/1     Running   0          101s
calico-node-vnj7f                          1/1     Running   0          101s
calico-node-vwnw4                          1/1     Running   0          101s
~~~



查看集群节点的状态是否为Ready

~~~powershell
kubectl get nodes

NAME           STATUS   ROLES    AGE     VERSION
k8s-master01   Ready    <none>   4h23m   v1.23.0
k8s-master02   Ready    <none>   4h22m   v1.23.0
k8s-master03   Ready    <none>   4h21m   v1.23.0
k8s-node01     Ready    <none>   4h21m   v1.23.0
~~~



### 部署CoreDNS

在k8s中，很多功能都需要用到DNS服务，例如：服务发现、负载均衡、有状态应用的访问等

文件地址：https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed

~~~powershell
cat >  coredns.yaml << "EOF"
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  - apiGroups:
    - discovery.k8s.io
    resources:
    - endpointslices
    verbs:
    - list
    - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local  in-addr.arpa ip6.arpa {
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. Default is 1.
  # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
         podAntiAffinity:
           preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchExpressions:
                   - key: k8s-app
                     operator: In
                     values: ["kube-dns"]
               topologyKey: kubernetes.io/hostname
      containers:
      - name: coredns
        image: coredns/coredns:1.8.4
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.96.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP

EOF
~~~



创建CoreDNS

~~~powershell
kubectl apply -f coredns.yaml
~~~



查看CoreDNS的Pod状态是否为Running

~~~powershell
# kubectl get pods -A
NAME                                       READY   STATUS
coredns-764bcc6b75-qmszv                   1/1     Running             
~~~



### Master节点设置污点

如果master节点只作为集群的管理节点，不希望运行用户应用容器，可以对master节点设置污点，来禁止Pod调度到master节点

```sh
#设置污点
kubectl taint node k8s-master01 k8s-master02 k8s-master03 master:NoSchedule


#查看污点
kubectl describe node k8s-master03 k8s-master02 k8s-master03 | grep -i taint
```



### 部署应用验证

~~~powershell
#部署nginx程序
kubectl create deployment nginx --image=nginx:1.20.0

#通过service暴露nginx可以被外部访问的NodePort端口
kubectl expose deployment nginx --port=80 --type=NodePort

#查看pod状态
kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-696649f6f9-j8zbj   1/1     Running   0          2m1s

#查看service状态
[root@master ~]# kubectl get service
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        44m
nginx        NodePort    10.103.195.31   <none>        80:32554/TCP   96s

#浏览器访问测试
http://192.168.0.10:32554/
~~~





# 第五章：Node节点加入现有集群

后续如需往k8s集群中添加工作节点，可以通过以下方式添加



#### 主机准备

| 主机IP       | 主机名     | 主机配置 | 角色     |
| ------------ | ---------- | -------- | -------- |
| 192.168.0.14 | k8s-node01 | 2C/4G    | 管理节点 |



#### 环境部署

k8s-node02配置集群本地解析

```sh
cat >> /etc/hosts << EOF
192.168.0.10 k8s-master01
192.168.0.11 k8s-master02
192.168.0.12 k8s-master03
192.168.0.13 k8s-node01
192.168.0.14 k8s-node02
192.168.0.70 k8s-ha1
192.168.0.71 k8s-ha2
EOF
```



集群现有节点增加新节点解析

~~~powershell
echo "192.168.0.14 k8s-node02" >> /etc/hosts
~~~



**开启bridge网桥过滤功能**

> 提示：只在k8s-node2节点执行

bridge(桥接) 是 Linux 系统中的一种虚拟网络设备，它充当一个虚拟的交换机，为集群内的容器提供网络通信功能，容器就可以通过这个 bridge 与其他容器或外部网络通信了。

~~~powershell
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

参数解释
net.bridge.bridge-nf-call-ip6tables = 1  //对网桥上的IPv6数据包通过iptables处理
net.bridge.bridge-nf-call-iptables = 1   //对网桥上的IPv4数据包通过iptables处理
net.ipv4.ip_forward = 1       //开启IPv4路由转发,来实现集群中的容器与外部网络的通信
~~~

~~~powershell
#由于开启bridge功能，需要加载br_netfilter模块来允许在bridge设备上的数据包经过iptables防火墙处理
modprobe br_netfilter && lsmod | grep br_netfilter

#...会输出以下内容
br_netfilter           22256  0
bridge                151336  1 br_netfilter

参数解释：
modprobe        //命令可以加载内核模块
br_netfilter    //模块模块允许在bridge设备上的数据包经过iptables防火墙处理
~~~

~~~powershell
#加载配置文件，使上述配置生效
sysctl -p /etc/sysctl.d/k8s.conf
~~~



**配置ipvs功能**

在k8s中Service有两种代理模式，一种是基于iptables的，一种是基于ipvs，两者对比ipvs负载均衡算法更加的灵活，且带有健康检查的功能，如果想要使用ipvs模式，需要手动载入ipvs模块。

`ipset` 和 `ipvsadm`  是两个与网络管理和负载均衡相关的软件包，在k8s代理模式中，提供多种负载均衡算法，如轮询（Round Robin）、最小连接（Least Connection）和加权最小连接（Weighted Least Connection）等；

~~~powershell
yum -y install ipset ipvsadm
~~~



将需要加载的ipvs相关模块写入到文件中

```powershell
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF


模块介绍
ip_vs         //提供负载均衡的模块,支持多种负载均衡算法,如轮询、最小连接、加权最小连接等
ip_vs_rr      //轮询算法的模块（默认算法）
ip_vs_wrr     //加权轮询算法的模块,根据后端服务器的权重值转发请求
ip_vs_sh      //哈希算法的模块,同一客户端的请求始终被分发到相同的后端服务器,保证会话一致性
nf_conntrack  //链接跟踪的模块,用于跟踪一个连接的状态,例如 TCP 握手、数据传输和连接关闭等
```



执行文件来加载模块

~~~powershell
chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack
~~~



**关闭SWAP分区**

为了保证 kubelet 正常工作，k8s强制要求禁用

~~~powershell
swapoff -a
sed -ri 's/.*swap.*/#&/' /etc/fstab
grep "swap" /etc/fstab
free -h
~~~



#### 准备docker

准备阿里云docker仓库（集群所有节点，不包括LB节点）

~~~powershell
wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
~~~



下载docker软件

~~~powershell
yum -y install docker-ce-20.10.9-3.el7.x86_64
~~~



设置docker的Cgroup用于对容器实现资源限制

~~~powershell
mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
~~~



启动docker服务

~~~powershell
systemctl enable docker
systemctl start docker
~~~



#### 部署kubelet

配置免密登录

> 提示：在k8s-master01节点拷贝公钥到k8s-node02节点

```sh
ssh-copy-id k8s-node02
```



在`k8s-master01`节点拷贝`kubelet、kube-proxy`程序

```sh
#提示：进入到程序存储路径在拷贝
scp kubelet kube-proxy k8s-node02:/usr/local/bin
```



在`k8s-node02`节点准备相关目录

```sh
#准备配置文件目录
mkdir -p /etc/kubernetes

#准备证书目录
mkdir -p /etc/kubernetes/ssl

#准备kubelet工作目录
mkdir -p /var/lib/kubelet

#准备日志目录
mkdir -p /var/log/kubernetes
```



在`k8s-master01`节点拷贝`kubelet.json、kubelet-bootstrap.kubeconfig、kubelet.kubeconfig、kubelet.service、ca.pem` 文件到 `k8s-node02` 节点

```sh
#提示：进入到文件的存储路径在执行拷贝
cd /root/work
scp kubelet-bootstrap.kubeconfig kubelet.json k8s-node02:/etc/kubernetes/
scp /etc/kubernetes/kubelet.kubeconfig k8s-node02:/etc/kubernetes/
scp kubelet.service k8s-node02:/usr/lib/systemd/system
scp ca.pem k8s-node02:/etc/kubernetes/ssl
```



在`k8s-node02`节点需要修改`kubelet.json`文件，修改其中的IP地址为本机

```sh
[root@k8s-node02 ~]# grep address /etc/kubernetes/kubelet.json
  "address": "192.168.0.14",
```



启动kubelet

```sh
systemctl daemon-reload
systemctl start kubelet
systemctl status kubelet
systemctl enable kubelet
```



在`k8s-master`节点查看集群节点信息

```sh
kubectl get node
NAME           STATUS     ROLES    AGE    VERSION
k8s-master01   Ready      <none>   2d2h   v1.23.0
k8s-master02   Ready      <none>   2d2h   v1.23.0
k8s-master03   Ready      <none>   2d2h   v1.23.0
k8s-node01     Ready      <none>   2d2h   v1.23.0
k8s-node02     NotReady   <none>   28s    v1.23.0
```



#### 部署kube-proxy

同步kube-proxy相关文件到k8s-node02节点

~~~shell
for k8s in k8s-node02
do
	scp kube-proxy.kubeconfig kube-proxy.yaml $k8s:/etc/kubernetes/
	scp kube-proxy.service $k8s:/usr/lib/systemd/system/
done
~~~



提示：`kube-proxy.yaml` 文件中 `bindAddress、healthzBindAddress、metricsBindAddress` 需要修改为当前主机IP地址。

~~~powershell
#k8s-node02节点
[root@k8s-node02 ~]# egrep "bindAddress|healthzBindAddress|metricsBindAddress" /etc/kubernetes/kube-proxy.yaml
...
bindAddress: 192.168.0.13
healthzBindAddress: 192.168.0.13:10256
metricsBindAddress: 192.168.0.13:10249
~~~



所有节点创建kube-proxy工作目录

```sh
mkdir /var/lib/kube-proxy
```



启动kube-proxy服务

~~~powershell
systemctl daemon-reload
systemctl start kube-proxy
systemctl status kube-proxy
systemctl enable kube-proxy
~~~


